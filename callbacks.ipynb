{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import re\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from fastprogress.fastprogress import format_time\n",
    "\n",
    "from exp.nb_utils import listify, is_listy, camel2snake\n",
    "from exp.nb_metrics import AvgLoss, AvgSmoothLoss\n",
    "from exp.nb_schedules import SchedExp\n",
    "from exp.nb_opt_utils import set_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train):\n",
    "        self.metrics, self.in_train = listify(metrics), in_train\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_loss = 0.\n",
    "        self.count = 0\n",
    "        self.total_metrics = [0.] * len(self.metrics)\n",
    "\n",
    "    @property\n",
    "    def all_stats(self): return [self.total_loss.item()] + self.total_metrics\n",
    "\n",
    "    @property\n",
    "    def avg_stats(self): return [s / self.count for s in self.all_stats]\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self.count:\n",
    "            return \"\"\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, learner):\n",
    "        bn = learner.xb.shape[0]\n",
    "        self.total_loss += learner.loss * bn\n",
    "        self.count += bn\n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.total_metrics[i] += m(learner.pred, learner.yb) * bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback():\n",
    "    _order = 0\n",
    "\n",
    "    def set_learner(self, learner): self.learner = learner\n",
    "\n",
    "    def __getattr__(self, k): return getattr(self.learner, k)\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        f = getattr(self, cb_name, None)\n",
    "        if f and f(): return True\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')\n",
    "\n",
    "\n",
    "class CancelTrainException(Exception): pass\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelValidException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
    "\n",
    "    def begin_fit(self):\n",
    "        met_names = ['loss'] + [m.__name__ for m in self.train_stats.metrics]\n",
    "        names = ['epoch'] + [f'train_{n}' for n in met_names] + [\n",
    "            f'valid_{n}' for n in met_names] + ['time']\n",
    "        #Write headers of table\n",
    "        self.logger(names)\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.learner)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        stats = [str(self.epoch)]\n",
    "        for o in [self.train_stats, self.valid_stats]:\n",
    "            stats += [f'{v:.6f}' for v in o.avg_stats]\n",
    "        stats += [format_time(time.time() - self.start_time)]\n",
    "        #Write row\n",
    "        self.logger(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainEvalCallback(Callback):\n",
    "    _order = -1\n",
    "\n",
    "    def begin_fit(self):\n",
    "        self.learner.cur_train_epoch_flt = 0.\n",
    "        self.learner.cur_train_iter = 0\n",
    "\n",
    "    def begin_train(self):\n",
    "        self.learner.cur_train_epoch = self.epoch\n",
    "        self.learner.pct_train = self.epoch / self.epochs\n",
    "        self.model.train()\n",
    "        self.learner.in_train = True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.learner.in_train = False\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.learner.cur_train_epoch_flt += 1./self.iters\n",
    "        self.learner.pct_train += 1./(self.iters*self.epochs)\n",
    "        self.learner.cur_train_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Recorder(Callback):\n",
    "    _order = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _maybe_item(t):\n",
    "        t = t.value\n",
    "        return t.item() if isinstance(t, Tensor) and t.numel() == 1 else t\n",
    "\n",
    "    def __init__(self, beta=0.98):\n",
    "        self.loss = AvgLoss()\n",
    "        self.smooth_loss = AvgSmoothLoss(beta=beta)\n",
    "\n",
    "    def begin_fit(self):\n",
    "        self.lrs, self.iters, self.losses, self.values = [], [], [], []\n",
    "        headers = [\"loss\"] + [m.name for m in self.metrics]\n",
    "        train_h = [\"train_{}\".format(h) for h in headers]\n",
    "        valid_h = [\"valid_{}\".format(h) for h in headers]\n",
    "        headers = [\"epoch\"] + train_h + valid_h\n",
    "        headers.append(\"time\")\n",
    "        self.metric_names = headers\n",
    "        self.smooth_loss.reset()\n",
    "#         self.logger(self.metric_names)\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.cancel_train, self.cancel_valid = False, False\n",
    "        self.start_epoch = time.time()\n",
    "        self.log = [getattr(self, 'epoch', 0)]\n",
    "\n",
    "    def begin_train(self):\n",
    "        # Reset except smooth_loss\n",
    "        for m in self._train_mets[1:]:\n",
    "            m.reset()\n",
    "\n",
    "    def begin_validate(self):\n",
    "        for m in self._valid_mets:\n",
    "            m.reset()\n",
    "\n",
    "    def after_train(self):\n",
    "        self.log += map(lambda x: self._maybe_item(x), self._train_mets)\n",
    "\n",
    "    def after_validate(self):\n",
    "        self.log += map(lambda x: self._maybe_item(x), self._valid_mets)\n",
    "\n",
    "    def after_cancel_train(self):\n",
    "        self.cancel_train = True\n",
    "\n",
    "    def after_cancel_validate(self):\n",
    "        self.cancel_validate = True\n",
    "\n",
    "    def after_epoch(self):\n",
    "        self.learner.final_record = self.log[1:].copy()\n",
    "        self.values.append(self.learner.final_record)\n",
    "\n",
    "        self.log.append(format_time(time.time() - self.start_epoch))\n",
    "        self.logger(self.log)\n",
    "        self.iters.append(self.smooth_loss.count)\n",
    "\n",
    "    def after_batch(self):\n",
    "        if len(self.yb) == 0: return\n",
    "        mets = self._train_mets if self.in_train else self._valid_mets\n",
    "        for met in mets: met.accumulate(self.learner)\n",
    "        if not self.in_train: return\n",
    "        self.lrs.append(self.opt.param_groups[-1]['lr'])\n",
    "        self.losses.append(self.smooth_loss.value)\n",
    "        self.learner.smooth_loss = self.smooth_loss.value\n",
    "\n",
    "    def plot_loss(self, skip_start=5, with_valid=True):\n",
    "        plt.plot(list(range(skip_start, len(self.losses))), self.losses[skip_start:], label='train')\n",
    "        if with_valid:\n",
    "            idx = (np.array(self.iters) < skip_start).sum()\n",
    "            plt.plot(self.iters[idx:], self.values[idx:], label='valid')\n",
    "            plt.legend()\n",
    "\n",
    "    def plot_lr_find(self, skip_end=5):\n",
    "        \"Plot the result of an LR Finder test (won't work if you didn't do `learn.lr_find()` before)\"\n",
    "        lrs = self.lrs if skip_end == 0 else self.lrs[:-skip_end]\n",
    "        losses = self.losses if skip_end == 0 else self.losses[:-skip_end]\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Learning Rate\")\n",
    "        ax.set_xscale('log')\n",
    "\n",
    "    def plot_sched(self, keys=None, figsize=None):\n",
    "        assert hasattr(self, 'hps'), '[ERROR] You must run ParamSchedCallback to plot the sched.'\n",
    "        keys = self.hps.keys() if keys is None else listify(keys)\n",
    "        rows,cols = (len(keys)+1)//2, min(2, len(keys))\n",
    "        figsize = figsize or (6*cols,4*rows)\n",
    "        _, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "        axs = axs.flatten() if len(keys) > 1 else listify(axs)\n",
    "        for p,ax in zip(keys, axs):\n",
    "            ax.plot(self.hps[p])\n",
    "            ax.set_ylabel(p)\n",
    "\n",
    "    @property\n",
    "    def _train_mets(self):\n",
    "        if getattr(self, 'cancel_train', False): return []\n",
    "        return [self.smooth_loss] + self.metrics\n",
    "\n",
    "    @property\n",
    "    def _valid_mets(self):\n",
    "        return [self.loss] + self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ParamScheduler(Callback):\n",
    "    _order = 0\n",
    "\n",
    "    def __init__(self, scheds):\n",
    "        self.scheds = scheds\n",
    "\n",
    "    def _update_val(self, percentage):\n",
    "        for n, f in self.scheds.items():\n",
    "            value = f(percentage)\n",
    "            set_hyper(self.opt, n, value)\n",
    "#             vs = listify(value)\n",
    "#             if len(vs) == 1: vs = vs * len(self.opt.param_groups)\n",
    "#             assert len(vs) == len(self.opt.param_groups), f\"Trying to set {len(vs)} values for {n} but there are {len(self.opt.param_groups)} parameter groups.\"\n",
    "#             for v,h in zip(vs, self.opt.param_groups):\n",
    "#                 h[n] = v\n",
    "\n",
    "    def begin_fit(self):\n",
    "        self.hps = {p:[] for p in self.scheds.keys()}\n",
    "\n",
    "    def begin_batch(self):\n",
    "        self._update_val(self.pct_train)\n",
    "\n",
    "    def after_batch(self):\n",
    "        for p in self.scheds.keys():\n",
    "            self.hps[p].append(self.opt.param_groups[-1][p])\n",
    "\n",
    "    def after_fit(self):\n",
    "        if hasattr(self.learner, 'recorder') and hasattr(self, 'hps'):\n",
    "            self.recorder.hps = self.hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ProgressCallback(Callback):\n",
    "    _order = 1\n",
    "\n",
    "    def begin_fit(self):\n",
    "        assert hasattr(self.learner, 'recorder')\n",
    "        self.mbar = master_bar(list(range(self.epochs)))\n",
    "        self._write_stats(self.recorder.metric_names)\n",
    "        self.learner.set_logger(self._write_stats)\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        if getattr(self, 'mbar', None): self.mbar.update(self.epoch)\n",
    "\n",
    "    def begin_train(self):\n",
    "        self._launch_pbar()\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self._launch_pbar()\n",
    "\n",
    "    def after_train(self):\n",
    "        self.pbar.on_iter_end()\n",
    "\n",
    "    def after_validate(self):\n",
    "        self.pbar.on_iter_end()\n",
    "\n",
    "    def after_batch(self):\n",
    "        self.pbar.update(self.iter + 1)\n",
    "        if hasattr(self, 'smooth_loss'):\n",
    "            self.pbar.comment = f\"{self.smooth_loss:.4f}\"\n",
    "\n",
    "    def after_fit(self):\n",
    "        if getattr(self, 'mbar', None):\n",
    "            self.mbar.on_iter_end()\n",
    "            delattr(self, 'mbar')\n",
    "        self.learner.logger = print\n",
    "\n",
    "    def _write_stats(self, log):\n",
    "        if getattr(self, 'mbar', None):\n",
    "            self.mbar.write([f\"{l:.6f}\" if isinstance(l, float) else str(l) for l in log], table=True)\n",
    "\n",
    "    def _launch_pbar(self):\n",
    "        self.pbar = progress_bar(self.dl, parent=getattr(self, 'mbar', None), leave=False)\n",
    "        self.pbar.update(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LRFinder(ParamScheduler):\n",
    "    _order = 1\n",
    "\n",
    "    def __init__(self, start_lr=1e-7, end_lr=10, n_iters=100, stop_div=True):\n",
    "        if is_listy(start_lr):\n",
    "            self.scheds = {\n",
    "                'lr': [SchedExp(s, e) for (s, e) in zip(start_lr, end_lr)]\n",
    "            }\n",
    "        else:\n",
    "            self.scheds = {\n",
    "                'lr': SchedExp(start_lr, end_lr)\n",
    "            }\n",
    "        self.n_iters, self.stop_div = n_iters, stop_div\n",
    "\n",
    "    def begin_fit(self):\n",
    "        super().begin_fit()\n",
    "        self.learner.save('_tmp')\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def begin_batch(self):\n",
    "        self._update_val(self.cur_train_iter / self.n_iters)\n",
    "\n",
    "    def after_batch(self):\n",
    "        super().after_batch()\n",
    "        if self.smooth_loss < self.best_loss:\n",
    "            self.best_loss = self.smooth_loss\n",
    "        if self.smooth_loss > 4*self.best_loss and self.stop_div:\n",
    "            raise CancelFitException()\n",
    "        if self.cur_train_iter >= self.n_iters:\n",
    "            raise CancelFitException()\n",
    "\n",
    "    def begin_validate(self):\n",
    "        raise CancelValidException()\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.learner.opt.zero_grad()\n",
    "        tmp_path = self.model_dir / '_tmp.pth'\n",
    "        if tmp_path.exists():\n",
    "            self.learner.load('_tmp')\n",
    "            os.remove(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CudaCallback(Callback):\n",
    "    _order = -1\n",
    "\n",
    "    def begin_fit(self): self.model.cuda()\n",
    "    def begin_batch(self): self.learner.xb,self.learner.yb = self.xb.cuda(), self.yb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted callbacks.ipynb to exp\\nb_callbacks.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py callbacks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
